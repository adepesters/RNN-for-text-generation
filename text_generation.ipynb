{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Using Multi-Layered RNN\n",
    "- [Introduction](#intro)\n",
    "- [Part 1: Data Preprocessing](#part1)\n",
    "- [Part 2: RNN Building](#part2)\n",
    "- [Part 3: RNN Training](#part3)\n",
    "- [Part 4: Text Generation](#part4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "\n",
    "In this project, I'll show how to build and train an RNN using Tensorflow to generate text. I train the RNN using a dataset consisting of the scripts from the first 8 seasons of the TV show Friends. \n",
    "\n",
    "The notebook consists of 4 parts: in Part 1 I'll preprocess the data. In Part 2, I'll build the network, which will then be trained in Part 3 and used to generate some text in Part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "## Part 1: Data Preprocessing\n",
    "\n",
    "The data preprocessing consists in tokenizing the punctuation, converting the text to lowercase and to integer codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and read text data\n",
    "data_dir = 'friends_script_season1-8.txt'\n",
    "input_file = os.path.join(data_dir)\n",
    "with open(input_file, \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Define punctuation for tokenization\n",
    "punctuation_dictionary = {'.': '||Period||', \n",
    "                            ',': '||Comma||',\n",
    "                            '\"': '||Quotation_mark||',\n",
    "                            ';': '||Semicolon||',\n",
    "                            '!': '||Exclamation_mark||',\n",
    "                            '?': '||Question_mark||',\n",
    "                            '(': '||Left_parenthesis||',\n",
    "                            ')': '||Right_parenthesis||',\n",
    "                            '--': '||Dash||',\n",
    "                            '\\n': '||Return||'}\n",
    "\n",
    "# Tokenize punctuation\n",
    "for key, token in punctuation_dictionary.items():\n",
    "    text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "# Convert text to lower case\n",
    "text = text.lower()\n",
    "\n",
    "# Split text into words\n",
    "text = text.split()\n",
    "\n",
    "# Assign codes to words \n",
    "vocab_to_int = {word: num for num, word in enumerate(set(text))}\n",
    "int_to_vocab = dict(enumerate(set(text)))\n",
    "\n",
    "# Convert full text into codes\n",
    "int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "pickle.dump((int_text, vocab_to_int, int_to_vocab, punctuation_dictionary), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "## Part 2: RNN Building\n",
    "\n",
    "We will build the RNN using 2 layers of LSTM cells. We will also embed the words before inputing them to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters:\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 400\n",
    "# Batch size\n",
    "batch_size = 512\n",
    "# RNN size\n",
    "rnn_size = 128\n",
    "# Number of layers in RNN\n",
    "lstm_layers = 2\n",
    "# Keep probability during dropout\n",
    "keep_prob = 0.6\n",
    "# Embedding dimension size\n",
    "embed_dim = 300\n",
    "# Sequence length for input and target\n",
    "seq_length = 50\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of epochs\n",
    "show_every_n_epochs = 10\n",
    "\n",
    "# Set saving directory\n",
    "save_dir = './save'\n",
    "\n",
    "# Save current parameters\n",
    "pickle.dump((seq_length, save_dir), open('params.p', 'wb'))\n",
    "\n",
    "# Reset any current graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define graph structure\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Define placeholders for input, target and learning rate\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    \n",
    "    # Embed text for input to RNN\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_text)\n",
    "\n",
    "    # Define RNN cell \n",
    "    def build_cell(rnn_size, keep_prob): # building lstm within build_cell necessary since tf 1.1\n",
    "            lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "\n",
    "            # Add dropout to the cell\n",
    "            drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "            return drop\n",
    "\n",
    "    # Stack up multiple LSTM layers\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size, keep_prob) for _ in range (lstm_layers)])\n",
    "\n",
    "    # Set initial state to 0s\n",
    "    initial_state = cell.zero_state(input_data_shape[0], tf.float32)\n",
    "\n",
    "    # Attribute name to initial state\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "\n",
    "    # Run RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "\n",
    "    # Attribute name to final state\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "\n",
    "    # Define logits\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "\n",
    "    # Pass logits through softmax\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Define loss function\n",
    "    cost = seq2seq.sequence_loss( # Use tf seq2seq.sequence_loss to compute cross-entropy loss between logits and targets sentences\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Clip gradients to prevent explosion\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "## Part 3: RNN Training\n",
    "\n",
    "Here we will define our batches of input and target text and train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0  train_loss = 6.071\n",
      "Epoch  10  train_loss = 4.866\n",
      "Epoch  20  train_loss = 4.492\n",
      "Epoch  30  train_loss = 4.285\n",
      "Epoch  40  train_loss = 4.059\n",
      "Epoch  50  train_loss = 3.878\n",
      "Epoch  60  train_loss = 3.785\n",
      "Epoch  70  train_loss = 3.721\n",
      "Epoch  80  train_loss = 3.668\n",
      "Epoch  90  train_loss = 3.634\n",
      "Epoch 100  train_loss = 3.607\n",
      "Epoch 110  train_loss = 3.577\n",
      "Epoch 120  train_loss = 3.564\n",
      "Epoch 130  train_loss = 3.549\n",
      "Epoch 140  train_loss = 3.538\n",
      "Epoch 150  train_loss = 3.519\n",
      "Epoch 160  train_loss = 3.517\n",
      "Epoch 170  train_loss = 3.504\n",
      "Epoch 180  train_loss = 3.490\n",
      "Epoch 190  train_loss = 3.485\n",
      "Epoch 200  train_loss = 3.472\n",
      "Epoch 210  train_loss = 3.462\n",
      "Epoch 220  train_loss = 3.475\n",
      "Epoch 230  train_loss = 3.468\n",
      "Epoch 240  train_loss = 3.456\n",
      "Epoch 250  train_loss = 3.453\n",
      "Epoch 260  train_loss = 3.449\n",
      "Epoch 270  train_loss = 3.455\n",
      "Epoch 280  train_loss = 3.443\n",
      "Epoch 290  train_loss = 3.461\n",
      "Epoch 300  train_loss = 3.448\n",
      "Epoch 310  train_loss = 3.447\n",
      "Epoch 320  train_loss = 3.445\n",
      "Epoch 330  train_loss = 3.436\n",
      "Epoch 340  train_loss = 3.442\n",
      "Epoch 350  train_loss = 3.435\n",
      "Epoch 360  train_loss = 3.433\n",
      "Epoch 370  train_loss = 3.443\n",
      "Epoch 380  train_loss = 3.434\n",
      "Epoch 390  train_loss = 3.439\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "# Get batches of input and target text data\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target. The returned batch variable is a np array of shape: \n",
    "    (total number of batches, 2 (i.e., input and target sequences), number of sequences per batch, \n",
    "    length of sequence). \n",
    "    \n",
    "    :param int_text: text with the words replaced by their ids\n",
    "    :param batch_size: number of sequences per batch\n",
    "    :param seq_length: length of each sequence (same for input and target)\n",
    "    :return: Numpy array of batches\n",
    "    \"\"\"\n",
    "    int_text = np.array(int_text)\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//characters_per_batch\n",
    "    int_text = int_text[:n_batches * characters_per_batch]\n",
    "    x_ = int_text[:]\n",
    "    tmp = int_text[1:]\n",
    "    y_ = np.append(tmp, x_[0:1]) # target y_ is = to input x_ offset by 1, with a last element that corresponds to first element of input x_ (as convention)\n",
    "    x_ = x_.reshape((batch_size, -1))\n",
    "    y_ = y_.reshape((batch_size, -1))\n",
    "    counter = 0\n",
    "    batch = np.zeros((n_batches, 2, batch_size, seq_length), dtype=int)\n",
    "    for n in range(0, x_.shape[1], seq_length):\n",
    "        x = np.array(x_[:, n:n+seq_length])\n",
    "        y = np.array(y_[:, n:n+seq_length])\n",
    "        batch[counter, 0, :, :] = x\n",
    "        batch[counter, 1, :, :] = y\n",
    "        counter += 1\n",
    "    return batch\n",
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "# Initialize session with defined graph\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        # Initialize state with first batch\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        # Train\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "        # Show training loss every <show_every_n_epochs> epochs\n",
    "        if epoch_i % show_every_n_epochs == 0:\n",
    "            print('Epoch {:>3}  train_loss = {:.3f}'.format(\n",
    "                epoch_i,\n",
    "                train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a>\n",
    "## Part 4: Text Generation\n",
    "\n",
    "Finalle, let's generate some text based on a prime input word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "joey: hey you told me!\n",
      "joey: oh yeah my mom!, who is when they’re happening doing that? all my brother.\n",
      "phoebe: how come the call as become a moment, why was my name?\n",
      "joey: because your? rules kinda been listening, and that beautiful is no heat, doesn’t up.\n",
      "opening credits\n",
      "[scene: central perk, phoebe, chandler, and chandler are sitting by joey to go and their coffee. ]\n",
      "chandler: hey!(she tilts the a treat.)\n",
      "all: no-no-no!! hey, all a beautiful word i have to find me. audition, yes. i mean i will just have a very fine reason about her and ann pretty.(they both start in of chandler's legs and other birds, monica still and both herself.) and it's gone.\n",
      "joey: correct, it's really funny. what should you?(to they and central boy and only enter. ]\n",
      "[scene: the theatre, monica and ross's joey, phoebe is there, rachel is with to use the newspapers. ]\n",
      "all: damn you!\n",
      "monica: yeah laid up fail\n",
      ", joey returns in the waiting entrance to the players. ] when do you tell him if you invited a troll burn position?(the crowd of tangled right along, he disgustedly knocks off, various system, and runs up.) quick pheebs are no!!\n",
      "chandler: roll hi!\n",
      "monica: it's, i don’t want you to say that, you'd see this…(rachel grabs her and realizes a bunny way and you know, and he tries pulling that his crotch.) ohhhhhhh, almost also.\n",
      "monica: will i understand, this is how you guys don't know.\n",
      "monica: nah.(to monica) who’s with you?\n",
      "chandler: well, space tratt come over. the random last time is frank’s geeks.\n",
      "chandler: who’s own?\n",
      "rachel: no. yeah no rachel: what then, let me one joey has a stew by the ninth grade, well that’s it, and you hadn’t uh. that that was true oh with the rest of the woman is the lot of evil juice. with me.\n",
      "mike: it’s a\n",
      "not director: i go i gotta, yes. the tour of your women. all not opening end up with me?\n",
      "phoebe:!\n",
      "dr. zane: you still think of a professional?\n",
      "joey: i won’t london, i'll-i'll tell him it's a new deal.\n",
      "monica: i…i know if you were thrown towards it. by coming.\n",
      "chandler: why?\n",
      "rachel: yes.\n",
      "monica sr: ok, the lead talking about that is a video-phone.(chandler laughs) i guess that switch would bad you’re always going at these years. it kinda hurts, which you happen. yeah. i was one for you just was here.\n",
      "joey: guys? do he want your office back?\n",
      "a casting late one?\n",
      "ross: me, yeah. but let's lift to get something something.\n",
      "pete: what are you talking about? answered on you and uh for this plan another, knicks?\n",
      "monica: no! wants me she was making hard a lot kinda thing for your digital bed!\n",
      "rachel: congratulations, no.(smiles carol’s face, lid dressed the cake. monica sees ross do lunges. very happy,,, better right at my altar in a date. would anybody say that he got out or a half-hour?\n",
      "(monica on the verge of tears which we had a pink cowboy block. ]\n",
      "chandler: hey.\n",
      "phoebe: you're kidding!\n",
      "[scene: joey and joey's, phoebe and monica are there as everyone enters. ]\n",
      "chandler:(on cell phone) hey, same city is on what we’re wearing?\n",
      "monica:(two reflection on her) ]\n",
      "chandler:(takes his sweater)\n",
      "rachel: what?! yeah! one backup!\n",
      "for the restaurant delivery is the nominees are: part fighter. ]\n",
      "joey: ahhhhh! believe uhh! because you’re hammered!\n",
      "mona:(reading them) excuse me?! who plays a present game?\n",
      "ross: no, take the duck you off huh?\n",
      "joey: yeah.(leaving dismissively which the lust feels to just a couple of garbage push.)\n",
      "rachel: of course you since kiss the\" counts?\n",
      "customer: yes, hilda.\n",
      "rachel: that changes.\n",
      "rachel: i'm not about anything right here.\n",
      "(he closes the door. rachel's beeper to his followed.)\n",
      "rachel: oh come on i’ve known his butt from deal.\n",
      "ross: what?\n",
      "rachel: hmm! i mean you don’t know what to do?\n",
      "joey: nope. that’s why what happens.\n",
      "rachel: i didn’t know about your joke over tonight, anyway, he would be rightie, and that’s i'm a little secure in my head.\n",
      "phoebe: ok, hypothetically, you don’t dance to work movies worked. it’s the first part, chandler is the first thing and y’know, then you're very good.(listens) y'know what the hell is about that?\n",
      "phoebe: yeah? no would no down.\n",
      "monica: really?\n",
      "monica: honey, sure you said you tribbiani!\n",
      "phoebe: would you be magical?\n"
     ]
    }
   ],
   "source": [
    "# Load previously saved parameters\n",
    "_, vocab_to_int, int_to_vocab, punctuation_dictionary = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, load_dir = pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "# Define words count of text to be generated\n",
    "gen_length = 1000\n",
    "# Define input word to the RNN\n",
    "prime_word = 'joey'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get tensors from loaded model\n",
    "    input_text = loaded_graph.get_tensor_by_name(name='input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name(name='initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name(name='final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name(name='probs:0')\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get prediction probabilities\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "                \n",
    "        # Chose predicted word according to probabilities    \n",
    "        word_idx = np.random.choice(range(len(int_to_vocab)), p=probabilities[dyn_seq_length-1])\n",
    "        pred_word = int_to_vocab[word_idx]\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Replace tokens by punctuation\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in punctuation_dictionary.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
